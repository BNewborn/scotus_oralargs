{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import Word\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "\n",
    "scotus_db = client.scotus_db\n",
    "scotus = scotus_db.scotus_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Text Analysis\n",
    "def sentiment_analysis(x):\n",
    "    return TextBlob(x).sentiment\n",
    "def polarity_analysis(x):\n",
    "    return TextBlob(x).polarity\n",
    "def subjectivity_analysis(x):\n",
    "    return TextBlob(x).subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Topics\n",
    "from sklearn import decomposition\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    '''\n",
    "    **Use like this**\n",
    "    df['lem_text'] = df['text'].apply(lemmatize_text)\n",
    "    '''\n",
    "    lemm = WordNetLemmatizer()\n",
    "    new_text = ''\n",
    "    for w in nltk.word_tokenize(text): \n",
    "        new_text += lemm.lemmatize(w) + ' '\n",
    "    return new_text\n",
    "\n",
    "def get_topics_cv(df):\n",
    "    count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        token_pattern=\"\\\\b[a-z][a-z][a-z]+\\\\b\",\n",
    "        min_df=1\n",
    "    )\n",
    "    count_vectorizer.fit(df)\n",
    "    # Create the term-document matrix\n",
    "    counts = count_vectorizer.transform(df)\n",
    "    # this gives us a [num_documents, num_features] sparse matrix\n",
    "    print(f\"Shape: {counts.shape}\")\n",
    "    n_topics = 10\n",
    "    lda = decomposition.LatentDirichletAllocation\\\n",
    "                        (n_components=n_topics, learning_method=\"online\"\n",
    "                        , max_iter=5, n_jobs=-1)\n",
    "\n",
    "    lda.fit(counts)\n",
    "    print(\"**Topics**\")\n",
    "    print_top_words(lda, count_vectorizer.get_feature_names(), 15)\n",
    "    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % (topic_idx+1)\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "def topics_by_year(year):\n",
    "    \n",
    "    cursor_func = scotus.find({\"term\": str(year)})\n",
    "    df_func = pd.DataFrame(list(cursor_func))\n",
    "\n",
    "    df_func['lem_text'] = df_func['text'].apply(lemmatize_text)\n",
    "    df_func_2=df_func['lem_text'].copy()\n",
    "    print(f\"SCOTUS Topics for Year: {year}\")\n",
    "    get_topics(df_func_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will run sentiment analysis on all cases in **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = scotus.find\\\n",
    "    \"term\":{\"$in\":['2006','2007','2008','2009',\\\n",
    "                               '2010','2011','2012','2013',\\\n",
    "                              '2014','2015','2016','2017']}\n",
    "          \n",
    "                   \n",
    "df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tb_polarity'] = df['text'].apply(polarity_analysis) #this takes a while\n",
    "print(\"polarity done\")\n",
    "df['tb_subjectivity'] = df['text'].apply(subjectivity_analysis)#this takes a while\n",
    "print(\"subjectivity done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = {\"lem_text\":'sum', 'tb_subjectivity':'sum', \"tb_polarity\":'sum'}\n",
    "\n",
    "df['lem_text']=df['text'].apply(lemmatize_text)\n",
    "df['case_term'] = df['case_name']+ ' ' + df['term']\n",
    "\n",
    "df = df.groupby(['case_term']).agg(agg_funcs).reset_index()\n",
    "case_term = list(df['case_term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm setting up another dataframe **df2** that will have the voting results by justice (who spoke during the case). We'll then join df2 (voting results) with df (text analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['lem_text']=df_2['text'].apply(lemmatize_text)\n",
    "df_2['case_term'] = df_2['case_name']+ ' ' + df_2['term']\n",
    "\n",
    "df_groupby = df_2.groupby(['case_term','speaker','voting_result']).max()\n",
    "\n",
    "df_groupby= df_groupby.reset_index()\n",
    "df_groupby = df_groupby[['case_term','speaker','voting_result']]\n",
    "\n",
    "df_pivot = df_groupby.pivot(index='case_term',columns='speaker',values='voting_result')\n",
    "\n",
    "df_votes_cases = df.merge(df_pivot,left_on='case_term',right_index=True)\n",
    "\n",
    "pd.to_pickle(df_votes_cases,'2006_2017_votes_AND_cases')\n",
    "df_votes_cases.sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T17:43:35.040575Z",
     "start_time": "2018-07-02T17:43:35.033500Z"
    }
   },
   "source": [
    "![Sample](df_votes_cases_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_votes_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_topic_words(model, feature_names, n_top_words):\n",
    "    topic_dict={}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_dict[topic_idx]=message\n",
    "    return topic_dict\n",
    "\n",
    "dict_topwords = dict_topic_words(lda,count_vectorizer.get_feature_names(),5)\n",
    "\n",
    "# now let's transform our documents to topic-space\n",
    "print(f\"shape before transforming to topic space: {counts.shape}\")\n",
    "\n",
    "doc_topics = lda.transform(counts)\n",
    "\n",
    "print(f\"shape after transforming to topic space: {doc_topics.shape}\")\n",
    "\n",
    "#Printed - pasted below\n",
    "# shape before transforming to topic space: (827, 15081)\n",
    "# shape after transforming to topic space: (827, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis - Count Vectorizer into LDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import Word\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "\n",
    "scotus_db = client.scotus_db\n",
    "scotus = scotus_db.scotus_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Text Analysis\n",
    "def sentiment_analysis(x):\n",
    "    return TextBlob(x).sentiment\n",
    "def polarity_analysis(x):\n",
    "    return TextBlob(x).polarity\n",
    "def subjectivity_analysis(x):\n",
    "    return TextBlob(x).subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Topics\n",
    "from sklearn import decomposition\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    '''\n",
    "    **Use like this**\n",
    "    df['lem_text'] = df['text'].apply(lemmatize_text)\n",
    "    '''\n",
    "    lemm = WordNetLemmatizer()\n",
    "    new_text = ''\n",
    "    for w in nltk.word_tokenize(text): \n",
    "        new_text += lemm.lemmatize(w) + ' '\n",
    "    return new_text\n",
    "\n",
    "def get_topics_cv(df):\n",
    "    count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        token_pattern=\"\\\\b[a-z][a-z][a-z]+\\\\b\",\n",
    "        min_df=1\n",
    "    )\n",
    "    count_vectorizer.fit(df)\n",
    "    # Create the term-document matrix\n",
    "    counts = count_vectorizer.transform(df)\n",
    "    # this gives us a [num_documents, num_features] sparse matrix\n",
    "    print(f\"Shape: {counts.shape}\")\n",
    "    n_topics = 10\n",
    "    lda = decomposition.LatentDirichletAllocation\\\n",
    "                        (n_components=n_topics, learning_method=\"online\"\n",
    "                        , max_iter=5, n_jobs=-1)\n",
    "\n",
    "    lda.fit(counts)\n",
    "    print(\"**Topics**\")\n",
    "    print_top_words(lda, count_vectorizer.get_feature_names(), 15)\n",
    "    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % (topic_idx+1)\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "def topics_by_year(year):\n",
    "    \n",
    "    cursor_func = scotus.find({\"term\": str(year)})\n",
    "    df_func = pd.DataFrame(list(cursor_func))\n",
    "\n",
    "    df_func['lem_text'] = df_func['text'].apply(lemmatize_text)\n",
    "    df_func_2=df_func['lem_text'].copy()\n",
    "    print(f\"SCOTUS Topics for Year: {year}\")\n",
    "    get_topics(df_func_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will run sentiment analysis on all cases in **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = scotus.find\\\n",
    "    \"term\":{\"$in\":['2006','2007','2008','2009',\\\n",
    "                               '2010','2011','2012','2013',\\\n",
    "                              '2014','2015','2016','2017']}\n",
    "          \n",
    "                   \n",
    "df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tb_polarity'] = df['text'].apply(polarity_analysis) #this takes a while\n",
    "print(\"polarity done\")\n",
    "df['tb_subjectivity'] = df['text'].apply(subjectivity_analysis)#this takes a while\n",
    "print(\"subjectivity done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = {\"lem_text\":'sum', 'tb_subjectivity':'sum', \"tb_polarity\":'sum'}\n",
    "\n",
    "df['lem_text']=df['text'].apply(lemmatize_text)\n",
    "df['case_term'] = df['case_name']+ ' ' + df['term']\n",
    "\n",
    "df = df.groupby(['case_term']).agg(agg_funcs).reset_index()\n",
    "case_term = list(df['case_term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm setting up another dataframe **df2** that will have the voting results by justice (who spoke during the case). We'll then join df2 (voting results) with df (text analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['lem_text']=df_2['text'].apply(lemmatize_text)\n",
    "df_2['case_term'] = df_2['case_name']+ ' ' + df_2['term']\n",
    "\n",
    "df_groupby = df_2.groupby(['case_term','speaker','voting_result']).max()\n",
    "\n",
    "df_groupby= df_groupby.reset_index()\n",
    "df_groupby = df_groupby[['case_term','speaker','voting_result']]\n",
    "\n",
    "df_pivot = df_groupby.pivot(index='case_term',columns='speaker',values='voting_result')\n",
    "\n",
    "df_votes_cases = df.merge(df_pivot,left_on='case_term',right_index=True)\n",
    "\n",
    "pd.to_pickle(df_votes_cases,'2006_2017_votes_AND_cases')\n",
    "df_votes_cases.sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T17:43:35.040575Z",
     "start_time": "2018-07-02T17:43:35.033500Z"
    }
   },
   "source": [
    "![Sample](df_votes_cases_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis - Count Vectorizer into LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_votes_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english',\n",
    "        token_pattern=\"\\\\b[a-z][a-z][a-z]+\\\\b\",\n",
    "        min_df=3,\n",
    "        max_df=.6\n",
    "    )\n",
    "df_text = df['lem_text']\n",
    "count_vectorizer.fit(df_text)\n",
    "# Create the term-document matrix\n",
    "counts = count_vectorizer.transform(df_text)\n",
    "# this gives us a [num_documents, num_features] sparse matrix\n",
    "\n",
    "print(f\"Shape: {counts.shape}\") \n",
    "#Shape: (827, 15081) 827 cases, 15,081 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "lda = decomposition.LatentDirichletAllocation(\n",
    "    n_components=30, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print_top_words(lda,count_vectorizer.get_feature_names(),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic #0: candidate political election speech money amendment race contribution vote public  \n",
    "Topic #1: officer police amendment fourth search car warrant arrest stop probable  \n",
    "Topic #2: duty honest mcnally bribe kickback service voir dire disclose intangible  \n",
    "Topic #3: class agency limitation board review jurisdiction notice percent jurisdictional epa  \n",
    "Topic #4: plan immunity power sovereign remedy clause official policy military suit  \n",
    "Topic #5: virginia commerce dormant cost requester staters privileges archive access recoup  \n",
    "Topic #6: child tribe parent indian tribal father mother reservation jurisdiction marriage  \n",
    "Topic #7: memorial monument cross display symbol establishment injunction plaque commandments erected  \n",
    "Topic #8: witness testimony cocaine confrontation clause trial prosecutor crawford prosecution report  \n",
    "Topic #9: article child standing custody convention country marriage sex iii residence  \n",
    "Topic #10: information public service amendment officer enforcement force activity speech report  \n",
    "Topic #11: fda drug trust information duty trustee manufacturer company regulation conflict  \n",
    "Topic #12: patent contract copyright infringement regulation agency agreement invention interpretation notice  \n",
    "Topic #13: tax money fraud bank transaction false property loss pay sec  \n",
    "Topic #14: contract jurisdiction agreement price foreign arbitration market power international treaty  \n",
    "Topic #15: plaintiff fee damage property money complaint injury attorney award pay  \n",
    "Topic #16: water compact montana user master river beneficial diversion special wyoming  \n",
    "Topic #17: employee employer union discrimination pay title employment service policy speech  \n",
    "Topic #18: jury trial error conviction review proceeding plea instruction defense lawyer  \n",
    "Topic #19: railroad carrier fcc tax property value transportation regulation truck rail  \n",
    "Topic #20: dna gene isolated myriad body scientist hansen molecule extracting amazon  \n",
    "Topic #21: california arbitration arbitrator trustee commissioner labor exemption debtor exempt volt  \n",
    "Topic #22: jersey income delaware month wharf disposable formula monthly distributor projected  \n",
    "Topic #23: land property water proximate river easement permit title park vessel  \n",
    "Topic #24: rico puerto senate recess president power session appointment enterprise member  \n",
    "Topic #25: commission mark carolina north arm amendment seed militia broadcast corps  \n",
    "Topic #26: charge notice sorna amendment percent filed race injury water arm  \n",
    "Topic #27: warning florida fbi miranda constitution interrogation presence attorney questioning convey  \n",
    "Topic #28: bankruptcy debt school debtor creditor chapter plan attorney student cost  \n",
    "Topic #29: offense crime sentence sentencing guideline element felony criminal risk conviction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_topic_words(model, feature_names, n_top_words):\n",
    "    topic_dict={}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_dict[topic_idx]=message\n",
    "    return topic_dict\n",
    "\n",
    "dict_topwords = dict_topic_words(lda,count_vectorizer.get_feature_names(),5)\n",
    "\n",
    "# now let's transform our documents to topic-space\n",
    "print(f\"shape before transforming to topic space: {counts.shape}\")\n",
    "\n",
    "doc_topics = lda.transform(counts)\n",
    "\n",
    "print(f\"shape after transforming to topic space: {doc_topics.shape}\")\n",
    "\n",
    "#Printed - pasted below\n",
    "# shape before transforming to topic space: (827, 15081)\n",
    "# shape after transforming to topic space: (827, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
